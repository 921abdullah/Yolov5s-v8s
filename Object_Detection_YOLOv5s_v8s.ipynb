{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV_jCPJ45lv4"
      },
      "source": [
        "# Lab 7: Object Detection\n",
        "\n",
        "Object detection is a fundamental task in computer vision that involves identifying and locating objects of interest within images or video frames. It plays a crucial role in various real-world applications, such as autonomous vehicles, video surveillance, image-based search, robotics, medical image analysis, and augmented reality. By accurately detecting and localizing objects, computer vision systems can better understand the visual world and make informed decisions based on the detected objects' positions, sizes, and relationships."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC67GXscByqW"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "We will use the Vehicle-OpenImages dataset from Roboflow. The dataset contains images of various vehicles in varied traffic conditions. These images have been collected from the Open Image dataset. The images are from varied conditions and scenes. It contains 5 classes in total. They are Car, Bus, Motorcycle, Truck, Ambulance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mzF9fS_-zG1"
      },
      "source": [
        "### Download and unzip the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YIbZRTN-5aE"
      },
      "source": [
        "For each of the train, validation, and test set folders within the dataset, you need to create two subfolders:\n",
        "\n",
        "1. \"images\": This folder should contain all the images corresponding to the specific set (train, validation, or test).\n",
        "2. \"labels\": This folder should have a \".txt\" file for every image in the \"images\" folder. Each label file will include one line per bounding box, with the bounding box information in the YOLO format as follows:\n",
        "\n",
        "object_class  x  y  width  height\n",
        "\n",
        "* \"object_class\": This is an integer representing the object's class. Start with 0 for the first class and increment by 1 for each additional unique class in the dataset.\n",
        "* \"x\" and \"y\": These values represent the normalized coordinates of the bounding box's center, with respect to the image width and height, respectively. Ensure that the coordinates are within the range of [0, 1].\n",
        "* \"width\" and \"height\": These values represent the normalized width and height of the bounding box, with respect to the image width and height, respectively. Ensure that these dimensions are within the range of [0, 1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeqBWoEwPdXv"
      },
      "outputs": [],
      "source": [
        "!gdown https://drive.google.com/file/d/16BZkCbPQNUwTEOYXsA2cPavUPQys0Acr/view?usp=sharing --fuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTfNHdFpPl3v"
      },
      "outputs": [],
      "source": [
        "!unzip /content/Vehicles-OpenImages.zip -d ./vehicles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCZW1pWn4sl6"
      },
      "source": [
        "# Annotation\n",
        "In order to annotate your own dataset use the labelimg tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1YeVg0PKN3h"
      },
      "source": [
        "# YOLOv5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DOlFsODUM0d"
      },
      "source": [
        "Clone the YOLOv5 Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMCtiHvEKQfU"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74wMfOlNUX6E"
      },
      "source": [
        "Install dependency from requirements.txt file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ev2cb8b8LzRG"
      },
      "outputs": [],
      "source": [
        "!pip install -r yolov5/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4_53-hsNbvM"
      },
      "source": [
        "## Training Options\n",
        "* img        : Size of image. The image is a square one. The original image is resized while maintaining the aspect ratio. The longer side of the image is resized to this number. The shorter side is padded with grey color.\n",
        "\n",
        "* batch: The batch size\n",
        "\n",
        "* epochs: Number of epochs to train for\n",
        "\n",
        "* data: Data YAML file that contains information about the\n",
        "dataset (path of images, labels)\n",
        "\n",
        "* workers: Number of CPU workers\n",
        "\n",
        "* cfg: Model architecture. There are 4 choices available: yolo5s.yaml, yolov5m.yaml, yolov5l.yaml, yolov5x.yaml. The size and complexity of these models increases in the ascending order and you can choose a model which suits the complexity of your object detection task. In case you want to work with a custom architecture, you will have to define a YAML file in the models folder specifying the network architecture.\n",
        "\n",
        "* weights: Pretrained weights you want to start training from. If you want to train from scratch, use --weights ' '\n",
        "\n",
        "* name: Various things about training such as train logs. Training weights would be stored in a folder named runs/train/name\n",
        "\n",
        "* hyp: YAML file that describes hyperparameter choices. For examples of how to define hyperparameters, see data/hyp.scratch.yaml. If unspecified, the file data/hyp.scratch.yaml is used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kjv9RVt_OYl9"
      },
      "source": [
        "## Data Config File\n",
        "\n",
        "* Details for the dataset you want to train your model on are defined by the data config YAML file. The following parameters have to be defined in a data config file:\n",
        "\n",
        "* train, test, and val: Locations of train, test, and validation images.\n",
        "\n",
        "* nc: Number of classes in the dataset.\n",
        "\n",
        "* names: Names of the classes in the dataset. The index of the\n",
        "classes in this list would be used as an identifier for the class names in the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjJr7EdkPFDv"
      },
      "source": [
        "## Hyperparameter Config File\n",
        "The hyperparameter config file helps us define the hyperparameters for our neural network. We are going to use the default one, data/hyp.scratch.yaml. This is what it looks like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEi-1KvPPGva"
      },
      "source": [
        "## Custom Network Architecture\n",
        "YOLO v5 also allows you to define your own custom architecture and anchors if one of the pre-defined networks doesn't fit the bill for you. For this you will have to define a custom weights config file. For this example, we use the the yolov5s.yaml. This is what it looks like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYZZNaWCPN7l"
      },
      "source": [
        "## Train Model from pretrained model\n",
        "The models range from nano to large as follows:\n",
        "\n",
        "\n",
        "*   yolov5n\n",
        "*   yolov5s\n",
        "* yolov5m\n",
        "* yolov5l\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpCj1Rgl2mH"
      },
      "source": [
        "### Important Note\n",
        "Please provide absolute paths for train and validation datasets in \"data.yaml\" file of you dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QOyaRV4nYl2"
      },
      "source": [
        "To train the model we use the \"train.py\" file and provide various arguments listed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZOX-hxZO7A8"
      },
      "outputs": [],
      "source": [
        "!python ./yolov5/train.py --img 640  --batch 32 --epochs 10 --data ./vehicles/data.yaml --weights 'yolov5n.pt' --name vehicles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkbDnVS_o_2a"
      },
      "source": [
        "## Perform validation on test/val set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HC1vIOHCnED-"
      },
      "source": [
        "To validate your model on the test set change the path of the \"val\" in your \"data.yaml\" file to the test set directory. If you want to validate it on validation set again, keep the validation path against the \"val\" variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kl0FgmzImzev"
      },
      "outputs": [],
      "source": [
        "!python ./yolov5/val.py --img 640  --batch 32 --data ./vehicles/data.yaml --weights '/content/yolov5/runs/train/vehicles2/weights/best.pt' --name vehicles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVmkVWOfpG1f"
      },
      "source": [
        "## Perform inference using trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWocKHSInYBG"
      },
      "source": [
        "To perform inference using our models, we use the \"detect.py\" file. By providing the source path to a folder, video or a single image, it will perform detection on all the images in the folder, or the complete video or the single image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqkfo8Y4orC9"
      },
      "outputs": [],
      "source": [
        "!python ./yolov5/detect.py --weights '/content/yolov5/runs/train/vehicles2/weights/best.pt' --source /content/vehicles/test/images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nbc3r9IID4VN"
      },
      "source": [
        "# YOLOv8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWvh7I-bUsVf"
      },
      "source": [
        "YOLOv8 is available in the form of a python package for ease of use. You can use tensorboard with each of the YOLO models given in this repo. You just have to provide the path of the specific directory containing the training experiments. In the cell below, before starting training we create the experiment directories ourselves so that we can start tensorboard before starting the model training for tracking model learning in real time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dEkh6g5AUyq"
      },
      "outputs": [],
      "source": [
        "!mkdir runs2\n",
        "!mkdir runs2/detect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG_xL1nhJx0G"
      },
      "outputs": [],
      "source": [
        "import tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/runs/detect\n",
        "# Don't worry if tensor board is not working for your. You may ignore this error and move on to the next cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NeebTMgi24p"
      },
      "outputs": [],
      "source": [
        "# This is the ultralytics library for using Ultralytics' various tools for object detection.\n",
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrymMqQuU6bt"
      },
      "source": [
        "## Train YOLOv8 Model\n",
        "The YOLOv8 can be trained, validated and used for inferenced by either using the CLI or its python API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSOCMhRxqXlw"
      },
      "source": [
        "1. Start model training of nano model from scratch using command line:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI7m2HxIjYwk"
      },
      "outputs": [],
      "source": [
        "!yolo task=detect mode=train model=yolov8n.yaml imgsz=640 data=/content/vehicles/data.yaml epochs=10 batch=8 name=yolov8n_scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjpqPIwRqcLW"
      },
      "source": [
        "2. Start training by using the \"ultralytics\" package in python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0mQgHxGzsEH"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "# Load the model.\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "# Training.\n",
        "results = model.train(\n",
        "   data='/content/vehicles/data.yaml',\n",
        "   imgsz=640,\n",
        "   epochs=10,\n",
        "   batch=64,\n",
        "   name='vehicles_pretrained')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCRPM2apqttv"
      },
      "source": [
        "## Validate YOLOv8 Model\n",
        "Validate the model on val/test. You need to change the path for the \"val\" variable to the test set in the \"data.yaml\" file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mYXI9JXrGqF"
      },
      "outputs": [],
      "source": [
        "# Load the model.\n",
        "model = YOLO('/content/yolov8n.pt')\n",
        "\n",
        "# Training.\n",
        "results = model.val(\n",
        "   data='/content/vehicles/data.yaml',\n",
        "   imgsz=640,\n",
        "   epochs=10,\n",
        "   batch=64,\n",
        "   name='vehicles_pretrained')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR69hUgbrDgN"
      },
      "source": [
        "## Perform Inference Using YOLOv8\n",
        "We use CLI for inferencing on the images in the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkDtYAvazpMz"
      },
      "outputs": [],
      "source": [
        "!yolo predict model=/content/yolov8n.pt source='/content/vehicles/test/images'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB03cbRdtYNl"
      },
      "source": [
        "You can also use the python code for this but it will return the predictions as a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkVeegi-rur1"
      },
      "outputs": [],
      "source": [
        "model = YOLO('/content/yolov8n.pt')\n",
        "results = model.predict(\n",
        "   source='/content/vehicles/test/images')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWbMvqeuG7_C"
      },
      "source": [
        "# Tasks:\n",
        "\n",
        "\n",
        "1.   Please fully train the YOLOv5s (small) and YOLOv8s (small) models on the provided dataset\n",
        "2.   Evaluate your models on the test sets and provide the mAP values.\n",
        "3.   Prepare a report in which you compare the performance of both the models. Please study the architectures of both YOLOV5 and YOLOV8 and illustrate these in your report.\n",
        "4. In your report, discuss which model is performing better on the test set and what can be the reason? If YOLOV8 is performing better, then what are the innovations that allow YOLOV8 to perform better than YOLOV5.\n",
        "5. Show predictions of both models on 4-5 test images in the report and compare them side by side.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***This is an individual lab.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wooc_RygdNKI"
      },
      "source": [
        "# Source\n",
        "\n",
        "https://github.com/Vision-At-SEECS/Pytorch_Labs/blob/main/Lab5_CV_Object_Detection.ipynb\n",
        "\n",
        "Prepared by: Bostan Khan, Team Lead, Machine Vision and Intelligent Systems Lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sSjUrTnJZzh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
